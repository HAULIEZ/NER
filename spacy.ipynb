{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf53051-30d1-4115-ab31-376f692a6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Step 1: Generate a base config for an NER model as a string\n",
    "config = \"\"\"\n",
    "[paths]\n",
    "train = null\n",
    "dev = null\n",
    "\n",
    "[system]\n",
    "gpu_allocator = \"pytorch\"\n",
    "\n",
    "[nlp]\n",
    "lang = \"xx\"  # Replace with language code if available\n",
    "pipeline = [\"ner\"]\n",
    "batch_size = 1000\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.ner]\n",
    "factory = \"ner\"\n",
    "\n",
    "[components.ner.model]\n",
    "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
    "\n",
    "[components.ner.model.tok2vec]\n",
    "@architectures = \"spacy.HashEmbedCNN.v2\"\n",
    "width = 96\n",
    "depth = 2\n",
    "embed_size = 2000\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "subword_features = true\n",
    "\n",
    "[training]\n",
    "dropout = 0.5\n",
    "optimizer = \"Adam\"\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "learn_rate = 0.001\n",
    "\n",
    "[training.batcher]\n",
    "@batchers = \"spacy.batch_by_words.v1\"\n",
    "size = 1000\n",
    "\n",
    "[initialize]\n",
    "vectors = null\n",
    "\"\"\"\n",
    "\n",
    "# Save config to file\n",
    "config_dir = \"modelv3\"\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "config_path = os.path.join(config_dir, \"config.cfg\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write(config)\n",
    "\n",
    "print(f\"Configuration saved to {config_path}\")\n",
    "\n",
    "# Step 2: Initialize a blank model or load a pre-trained one if available\n",
    "nlp = spacy.blank(\"xx\")\n",
    "\n",
    "# Add NER pipeline to the model\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Load the JSON file containing your training data\n",
    "with open(\"annotated.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Add entity labels\n",
    "for label in data['classes']:\n",
    "    ner.add_label(label)\n",
    "\n",
    "# Prepare training data (annotations) from the dataset\n",
    "annotations = [item for item in data['annotations'] if item]  # Filter out None values\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_annotations, test_annotations = train_test_split(annotations, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the training data to spaCy's Example format\n",
    "train_examples = []\n",
    "for item in train_annotations:\n",
    "    if item and isinstance(item, list) and len(item) == 2:  # Check for correct format\n",
    "        text, annotations = item\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)  # Convert to spaCy's Example format\n",
    "        train_examples.append(example)\n",
    "    else:\n",
    "        print(f\"Skipping invalid item: {item}\")\n",
    "\n",
    "# train the model\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Number of training iterations (epochs)\n",
    "n_iter = 30\n",
    "for epoch in range(n_iter):\n",
    "    losses = {}\n",
    "    # Shuffle the examples before each iteration\n",
    "    for example in train_examples:\n",
    "        nlp.update([example], sgd=optimizer, losses=losses)\n",
    "    print(f\"Epoch {epoch+1}/{n_iter} Losses: {losses}\")\n",
    "\n",
    "# Save the trained model\n",
    "output_dir = config_dir\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "# Load the trained model\n",
    "nlp = spacy.load(output_dir)\n",
    "\n",
    "#Finally, use the spacy train and config file to train the model on the prepared data in spacy format as shown below\n",
    "#! python -m spacy train config.cfg --output ./ --paths.train ./chichewa_training_data.spacy \n",
    "\n",
    "# Test the model\n",
    "doc = nlp(\"Matenda a shuga ndi matenda omwe amadziwika ndi kuchuluka kwa shuga m'magazi, kapena kuchuluka kwa shuga m'magazi.\")\n",
    "\n",
    "# Check if any entities are recognized\n",
    "if doc.ents:\n",
    "    # If entities are found, print them\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "else:\n",
    "    # If no entities are found, print a message\n",
    "    print(\"No entities recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea46f3-cb55-4e25-8edc-77a7a6655015",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
